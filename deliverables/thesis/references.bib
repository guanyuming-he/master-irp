@article{first.bis.school.1,
  title={The business school in America: a century goes by},
  author={Spender, John-Christopher},
  journal={The future of business schools: Scenarios and strategies for},
  pages={9--18},
  year={2020}
}

@book{first.bis.school.2,
  title={The pragmatic imagination: A history of the Wharton School, 1881-1981},
  author={Sass, Steven A},
  year={2016},
  publisher={University of Pennsylvania Press}
}

@book{case.method.origin.1,
  title={Learning by the case method},
  author={Hammond, John S},
  year={1980},
  publisher={Harvard Business School Boston, MA}
}

@inproceedings{case.method.origin.2,
  title={The case method as invented tradition: revisiting harvard's history to reorient management education},
  author={Bridgman, Todd and Cummings, Stephen and McLaughlin, Colm},
  booktitle={Academy of Management Proceedings},
  volume={2015},
  number={1},
  pages={11637},
  year={2015},
  organization={Academy of Management Briarcliff Manor, NY 10510}
}

@article{case.method.support.1,
  title={Case studies in business education: an investigation of a learner-friendly approach},
  author={V{\'\i}te{\v{c}}kov{\'a}, Kl{\'a}ra and Cramer, Tobias and Pilz, Matthias and T{\"o}gel, Janine and Albers, Sascha and van den Oord, Steven and Rachwa{\l}, Tomasz},
  journal={Journal of International Education in Business},
  volume={18},
  number={2},
  pages={149--176},
  year={2025},
  publisher={Emerald Publishing Limited}
}

@article{case.method.support.2,
author = {Kevin M. Bonney},
title = {Case Study Teaching Method Improves Student Performance and Perceptions of Learning Gains},
journal = {Journal of Microbiology \&amp; Biology Education},
volume = {16},
number = {1},
pages = {21-28},
year = {2015},
doi = {10.1128/jmbe.v16i1.846},

URL = {https://journals.asm.org/doi/abs/10.1128/jmbe.v16i1.846},
eprint = {https://journals.asm.org/doi/pdf/10.1128/jmbe.v16i1.846}
}

@article{case.method.support.3,
  title={Teaching business as business: The role of the case method in the constitution of management as a science-based profession},
  author={Lusoli, Alberto},
  journal={Journal of Management History},
  volume={26},
  number={2},
  pages={277--290},
  year={2020},
  publisher={Emerald Publishing Limited}
}

@article{case.method.adoption.1,
  title={The use of case studies in undergraduate business administration},
  author={Trejo-Pech, Carlos JO and White, Susan},
  journal={Revista de administra{\c{c}}{\~a}o de empresas},
  volume={57},
  pages={342--356},
  year={2017},
  publisher={SciELO Brasil}
}

@article{case.method.adoption.2,
  title={Supporting the adoption of business case studies in ESP instruction through technology},
  author={Eckhaus, Rebekka},
  journal={Asian ESP Journal},
  volume={14},
  pages={280--281},
  year={2018}
}

@article{case.method.limit.1,
  title={Case Method of Teaching in Management Education},
  author={Shah, Binod},
  journal={Research Gate},
  year={2019}
}

@misc{case.method.limit.2,
  title={Teaching and the Case Method Harvard Business School},
  author={Christensen, C Roland},
  howpublished={\url{https://www.hbs.edu/teaching/case-method/Pages/default.aspx}},
  year={1987}
}

@article{case.method.limit.3,
author = {Sandeep Puri},
title = {Effective learning through the case method},
journal = {Innovations in Education and Teaching International},
volume = {59},
number = {2},
pages = {161--171},
year = {2022},
publisher = {SRHE Website},
doi = {10.1080/14703297.2020.1811133},
URL = { 
        https://doi.org/10.1080/14703297.2020.1811133
},
eprint = { 
        https://doi.org/10.1080/14703297.2020.1811133
}
}

@misc{case.method.limit.4,
author = {Clark, Tom},
title = {Case method in the digital age: how might new technologies shape
		 experiential learning and real-life story telling?},
howpublished = {LSE Impact of Social Sciences},
year = {2016}
}

@article{case.method.limit.5,
  title={Guidelines for using case studies in the teaching-learning process.},
  author={McFarlane, Donovan A},
  journal={College Quarterly},
  volume={18},
  number={1},
  pages={n1},
  year={2015},
  publisher={ERIC}
}

@ARTICLE{history.telegraph.1,
  author={Schwartz, Mischa and Hochfelder, David},
  journal={IEEE Communications Magazine}, 
  title={Two controversies in the early history of the telegraph}, 
  year={2010},
  volume={48},
  number={2},
  pages={28-32},
  keywords={History;Telegraphy;Biographies;Books;Writing;Electromagnets;Lightning;Libraries;Biological materials;Art},
  doi={10.1109/MCOM.2010.5402659}}

@book{history.telegraph.2,
  title={Makers of the Telegraph: Samuel Morse, Ezra Cornell and Joseph Henry},
  author={Lifshitz, Kenneth B},
  year={2017},
  publisher={McFarland}
}

@misc{first.telegraph.msg,
	title={First telegraph message},
	author={Morse, Samuel Finley Breese},
	year={1844},
	month={May},
	howpublished={Retrieved from the Library of Congress,
			   \url{https://www.loc.gov/item/mcc.019/}}
}

@ARTICLE{history.telephone.1,
  author={Watson, Thomas A.},
  journal={Proceedings of the American Institute of Electrical Engineers}, 
  title={How Bell invented the telephone}, 
  year={1915},
  volume={34},
  number={8},
  pages={1503-1513},
  keywords={Receivers;Technological innovation;Transmitters;Harmonic analysis;Vibrations;Steel;Electricity},
  doi={10.1109/PAIEE.1915.6590775}}

@article{history.telephone.2,
author = {J.E. Flood },
title = {Alexander Graham Bell and the invention of the telephone},
journal = {Electronics and Power},
volume = {22},
issue = {3},
pages = {159-162},
year = {1976},
doi = {10.1049/ep.1976.0077},

URL = {https://digital-library.theiet.org/doi/abs/10.1049/ep.1976.0077},
eprint = {https://digital-library.theiet.org/doi/pdf/10.1049/ep.1976.0077}
,
    abstract = { Mr. Watson was stationed in one room with the receiving instrument. He pressed his ear closely against S [the receiving instrument] and closed his other ear with his hand. The transmitting instrument was placed in another room and the doors of both rooms were closed. I [Graham Bell] then shouted into M [the horn of the transmitting instrument] the following sentence: “Mr. Watson-Come here-I want to see you.” The story of the invention of the telephone is a fascinating one: it contains romance; a rise from poverty to riches; royal occasions; a race between inventors; the defeat of a giant corporation by the small man; and success brought about by a combination of genius, hard work and happy coincidences }
}

@ARTICLE{history.wireless.1,
  author={Falciasecca, Gabriele},
  journal={IEEE Antennas and Propagation Magazine}, 
  title={Marconi's Early Experiments in Wireless Telegraphy, 1895}, 
  year={2010},
  volume={52},
  number={6},
  pages={220-221},
  keywords={Wireless communication;Telegraphy;Testing;History},
  doi={10.1109/MAP.2010.5723274}}

@article{history.first.atlantic.broadcast,
author = {Beynon, W. J. G.},
title = {Marconi, radio waves, and the ionosphere},
journal = {Radio Science},
volume = {10},
number = {7},
pages = {657-664},
doi = {https://doi.org/10.1029/RS010i007p00657},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/RS010i007p00657},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/RS010i007p00657},
abstract = {The life story of Marconi is too well documented to need detailed repetition, but there are certain facts about his early years which bear directly upon his quite remarkable success and indeed which relate to the early development of the subject of wireless telegraphy. Anyone looking at the early history of radio wave propagation and of Marconi's major role in it, cannot fail to be impressed with certain facts. Thus it is surely remarkable that in February 1896 when he first took his black wireless boxes to England, Marconi was only 21 years old and it is to be remembered that not only was he a very young man but he had had very little formal training in science. The second remarkable fact is that on 2 June 1896, i.e., within a few weeks of arriving in England (he was now just 22) he filed with the British Patent Office a provisional patent specification (the first of many) to safeguard his commercial interests in wireless communication. The third striking fact is that just one year later, in the summer of 1897, at the age of 23 he and his friends formed the Wireless Telegraph and Signal Company Limited (soon to be renamed the Marconi Wireless Telegraph Company) with an initial capital of £100,000. The shares in the new company were quickly subscribed; Marconi himself received £15,000 in cash and also held a large number of the shares.},
year = {1975}
}

@article{first.voice.broadcast,
author = {Elliot N. Sivowitch},
title = {A technological survey of broadcasting's “pre‐history,” 1876–1920},
journal = {Journal of Broadcasting},
volume = {15},
number = {1},
pages = {1--20},
year = {1970},
publisher = {Routledge},
doi = {10.1080/08838157009363620},
URL = { 
        https://doi.org/10.1080/08838157009363620
},
eprint = { 
        https://doi.org/10.1080/08838157009363620
}
}

@article{wireless.weakness.1,
author = {Brian N. Hall},
title ={The British Army and Wireless Communication, 1896–1918},
journal = {War in History},
volume = {19},
number = {3},
pages = {290-321},
year = {2012},
doi = {10.1177/0968344512444505},
URL = { 
        ttps://doi.org/10.1177/0968344512444505
},
eprint = { 
        https://doi.org/10.1177/0968344512444505
},
}


@article{wireless.weakness.2,
 ISSN = {0003049X},
 URL = {http://www.jstor.org/stable/24640213},
 author = {JONATHAN REED WINKLER},
 journal = {Proceedings of the American Philosophical Society},
 number = {2},
 pages = {162--168},
 publisher = {[American Philosophical Society, American Philosophical Association]},
 title = {Telecommunications in World War I},
 urldate = {2025-06-04},
 volume = {159},
 year = {2015}
}
	
@ARTICLE{wireless.weakness.3,
  author={Alexanderson, E. F. W.},
  journal={Proceedings of the American Institute of Electrical Engineers}, 
  title={Transatlantic radio communication}, 
  year={1919},
  volume={38},
  number={10},
  pages={1077-1093},
  keywords={Radio communication;Europe;Receivers;Antennas;Signal resolution;Modulation;Tuning},
  doi={10.1109/PAIEE.1919.6591907}}

@article{hartley.log.information,
author = {Hartley, R. V. L.},
title = {Transmission of Information},
journal = {Bell System Technical Journal},
volume = {7},
number = {3},
pages = {535-563},
doi = {https://doi.org/10.1002/j.1538-7305.1928.tb01236.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1928.tb01236.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1928.tb01236.x},
year = {1928}
}

@article{shannon.theory.communication,
author = {Shannon, C. E.},
title = {A mathematical theory of communication},
year = {2001},
issue_date = {January 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1559-1662},
url = {https://doi.org/10.1145/584091.584093},
doi = {10.1145/584091.584093},
journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
month = jan,
pages = {3–55},
numpages = {53}
}

@ARTICLE{history.modulation,
  author={Heising, Raymond A.},
  journal={Proceedings of the IRE}, 
  title={Modulation Methods}, 
  year={1962},
  volume={50},
  number={5},
  pages={896-901},
  keywords={Optical transmitters;Telephony;Radio transmitters;Sparks;Alternators;Microphones;Radio control;Art;Radio broadcasting;Circuits},
  doi={10.1109/JRPROC.1962.288368}}

@ARTICLE{history.information.retrieval,
  author={Sanderson, Mark and Croft, W. Bruce},
  journal={Proceedings of the IEEE}, 
  title={The History of Information Retrieval Research}, 
  year={2012},
  volume={100},
  number={Special Centennial Issue},
  pages={1444-1451},
  keywords={Search engines;Information retrieval;History;Data storage systems;Internet;Web search;Information resources;Information services;History;information retrieval;ranking algorithms},
  doi={10.1109/JPROC.2012.2189916}}

@inproceedings{mooers.info.ret.term,
  title={The theory of digital handing of non-numerical information and its implications to machine economics},
  author={Mooers, Calvin},
  booktitle={Proceedings of the meeting of the Association for Computing Machinery at Rutgers University},
  year={1950}
}

@ARTICLE{early.info.systems,
  author={Griffiths, J.-M. and King, D.W.},
  journal={IEEE Annals of the History of Computing}, 
  title={US information retrieval system evolution and evaluation (1945-1975)}, 
  year={2002},
  volume={24},
  number={3},
  pages={35-55},
  keywords={Information retrieval;Computer networks;Distributed computing;Databases;Optical computing;Technological innovation;Boolean functions;Encoding;Optical devices;Sorting},
  doi={10.1109/MAHC.2002.1024761}}


@article{history.internet,
author = {Leiner, Barry M. and Cerf, Vinton G. and Clark, David D. and Kahn, Robert E. and Kleinrock, Leonard and Lynch, Daniel C. and Postel, Jon and Roberts, Larry G. and Wolff, Stephen},
title = {A brief history of the internet},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/1629607.1629613},
doi = {10.1145/1629607.1629613},
abstract = {This paper was first published online by the Internet Society in December 20031 and is being re-published in ACM SIGCOMM Computer Communication Review because of its historic import. It was written at the urging of its primary editor, the late Barry Leiner. He felt that a factual rendering of the events and activities associated with the development of the early Internet would be a valuable contribution. The contributing authors did their best to incorporate only factual material into this document. There are sure to be many details that have not been captured in the body of the document but it remains one of the most accurate renderings of the early period of development available.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = oct,
pages = {22–31},
numpages = {10},
keywords = {internet, history}
}

@article{history.search.engines,
  title={History of search engines},
  author={Seymour, Tom and Frantsvog, Dean and Kumar, Satheesh and others},
  journal={International Journal of Management \& Information Systems (IJMIS)},
  volume={15},
  number={4},
  pages={47--58},
  year={2011},
  publisher={Clute Institute}
}

@Inbook{history.internet.search.engines,
author="Okunev, Rhoda",
title="History of the Internet, Search Engines, and More",
bookTitle="The Psychology of Evolving Technology: How Social Media, Influencer Culture and New Technologies are Altering Society",
year="2023",
publisher="Apress",
address="Berkeley, CA",
pages="9--16",
abstract="This chapter will review the internet and the WWW, search engines, email, word processors, Wi-Fi and texting. After the invent of the computer many inventions were developed that made life easier while using them. The internet, WWW and Wi-Fi enabled computers to speak to each other and share information. Search engines allowed users to see details of facts and figures that were on the internet instead of having to search through encyclopedias. The word processor made writing, computing and editing simpler to manage than a typewriter. Email and texting allowed the user to relay messages without having to make a call. In October 1957, when the Soviets launched Sputnik, the first manmade satellite, into space, the United States panicked. Americans started to realize we needed more science and technology courses and majors at universities. The goal of the National Aeronautics and Space Administration (NASA) and the Department of Defense's Advanced Research Project Agency (ARPA) was to develop more space-age technology such as rockets and computers to rival the Soviet threat. In the 1960s, during the Cold War, MIT, Rand, and others recommended building computers that could speak to each other to enable leaders to communicate even if the telephone system was destroyed. In 1968, Advanced Research Projects Agency Network (ARPAnet) used a technique developed by MIT called packet switching. This technique was the start of computers communicating but did not work very well. It did lay the groundwork for the Internet, though.",
isbn="978-1-4842-8686-9",
doi="10.1007/978-1-4842-8686-9_2",
url="https://doi.org/10.1007/978-1-4842-8686-9_2"
}

@inproceedings{search.engine.impact.1,
author = {Cho, Junghoo and Roy, Sourashis},
title = {Impact of search engines on page popularity},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988676},
doi = {10.1145/988672.988676},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {20–29},
numpages = {10},
keywords = {change in pagerank, pagerank, random surfer model, search engine's impact, web evolution},
location = {New York, NY, USA},
series = {WWW '04}
}

@article{search.engine.impact.2,
  title={The impact of search engines in the world today},
  author={Israel, Olagoke Olawale and Samson, Olatunji Olusoji},
  journal={International Journal of Management, IT and Engineering},
  volume={8},
  number={3},
  pages={10--22},
  year={2018},
  publisher={International Journals of Multidisciplinary Research Academy}
}

@ARTICLE{history.global.cables.1,
  author={Schwartz, Mischa and Hayes, Jeremiah},
  journal={IEEE Communications Magazine}, 
  title={A history of transatlantic cables}, 
  year={2008},
  volume={46},
  number={9},
  pages={42-48},
  keywords={History;Communication cables;Underwater cables;Cable insulation;Telephony;Telegraphy;Oceans;Marine technology},
  doi={10.1109/MCOM.2008.4623705}}


@article{llm.impact.1,
  title={Unraveling the landscape of large language models: a systematic review and future perspectives},
  author={Ding, Qinxu and Ding, Ding and Wang, Yue and Guan, Chong and Ding, Bosheng},
  journal={Journal of Electronic Business \& Digital Economics},
  volume={3},
  number={1},
  pages={3--19},
  year={2023},
  publisher={Emerald Publishing Limited}
}

@inproceedings{llm.info.ret.1,
author = {Liu, Zheng and Zhou, Yujia and Zhu, Yutao and Lian, Jianxun and Li, Chaozhuo and Dou, Zhicheng and Lian, Defu and Nie, Jian-Yun},
title = {Information Retrieval Meets Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641299},
doi = {10.1145/3589335.3641299},
abstract = {The advent of large language models (LLMs) presents both opportunities and challenges for the information retrieval (IR) community. On one hand, LLMs will revolutionize how people access information, meanwhile the retrieval techniques can play a crucial role in addressing many inherent limitations of LLMs. On the other hand, there are open problems regarding the collaboration of retrieval and generation, the potential risks of misinformation, and the concerns about cost-effectiveness. To seize the critical moment for development, it calls for the joint effort from academia and industry on many key issues, including identification of new research problems, proposal of new techniques, and creation of new evaluation protocols. It has been one year since the launch of ChatGPT in November last year, and the entire community is currently undergoing a profound transformation in techniques. Therefore, this workshop will be a timely venue to exchange ideas and forge collaborations. The organizers, committee members, and invited speakers are composed of a diverse group of researchers coming from leading institutions in the world. This event will be made up of multiple sessions, including invited talks, paper presentations, hands-on tutorials, and panel discussions. All the materials collected for this workshop will be archived and shared publicly, which will present a long-term value to the community.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1586–1589},
numpages = {4},
keywords = {information retrieval, large language models, question answering, ranking, retrieval-augmented generation, search},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{llm.info.ret.2,
author = {Zhai, ChengXiang},
title = {Large Language Models and Future of Information Retrieval: Opportunities and Challenges},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657848},
doi = {10.1145/3626772.3657848},
abstract = {Recent years have seen great success of large language models (LLMs) in performing many natural language processing tasks with impressive performance, including tasks that directly serve users such as question answering and text summarization. They open up unprecedented opportunities for transforming information retrieval (IR) research and applications. However, concerns such as halluciation undermine their trustworthiness, limiting their actual utility when deployed in real-world applications, especially high-stake applications where trust is vital. How can we both exploit the strengths of LLMs and mitigate any risk caused by their weaknesses when applying LLMs to IR? What are the best opportunities for us to apply LLMs to IR? What are the major challenges that we will need to address in the future to fully exploit such opportunities? Given the anticipated growth of LLMs, what will future information retrieval systems look like? Will LLMs eventually replace an IR system? In this perspective paper, we examine these questions and provide provisional answers to them. We argue that LLMs will not be able to replace search engines, and future LLMs would need to learn how to use a search engine so that they can interact with a search engine on behalf of users. We conclude with a set of promising future research directions in applying LLMs to IR.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {481–490},
numpages = {10},
keywords = {conversational information access, information retrieval models, intelligent agent, large language models, search engines},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@misc{llm.info.ret.3,
  title={Use of LLM for Methods of Information Retrieval},
  author={Gavilanes, Jose and Bozhilov, Yancho and Dodeja, Ujjwal and Valtas, Georgios and Badrajan, Alen},
  year={2023},
  publisher={Enschede}
}

@article{llm.power.1,
author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
title = {Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3649506},
doi = {10.1145/3649506},
abstract = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at . An LLMs evolutionary tree, editable yet regularly updated, can be found at  .},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {160},
numpages = {32},
keywords = {Large language models, neural language processing, practical guide, ChatGPT}
}

@article{llm.power.2,
  title={Large language models (LLMs): survey, technical frameworks, and future challenges},
  author={Kumar, Pranjal},
  journal={Artificial Intelligence Review},
  volume={57},
  number={10},
  pages={260},
  year={2024},
  publisher={Springer}
}

@INPROCEEDINGS{llm.limit.1,
  author={Sanu, Erin and Amudaa, T Keerthi and Bhat, Prasiddha and Dinesh, Guduru and Kumar Chate, Apoorva Uday and P, Ramakanth Kumar},
  booktitle={2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)}, 
  title={Limitations of Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Large language models;Perturbation methods;Training data;Medical services;Predictive models;Natural language processing;Data models;Real-time systems;Reliability;Large Language Models;natural language processing;Hallucinations;Biases;Adversarial Attacks;Ethical Implications;vulnerabilities},
  doi={10.1109/CSITSS64042.2024.10817070}}

 
@incollection{llm.limit.2,
  title={Fundamental limitations of generative llms},
  author={Kucharavy, Andrei},
  booktitle={Large Language Models in Cybersecurity: Threats, Exposure and Mitigation},
  pages={55--64},
  year={2024},
  publisher={Springer Nature Switzerland Cham}
}

@inproceedings{llm.limit.3,
  title={Stochastic LLMs do not understand language: towards symbolic, explainable and ontologically based LLMs},
  author={Saba, Walid S},
  booktitle={International conference on conceptual modeling},
  pages={3--19},
  year={2023},
  organization={Springer}
}

@article{llm.hallucination.1,
author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3703155},
doi = {10.1145/3703155},
abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {42},
numpages = {55},
keywords = {Large Language Models, Hallucination, Factuality, Faithfulness}
}

@misc{llm.hallucination.2,
      title={A Survey of Hallucination in Large Foundation Models}, 
      author={Vipula Rawte and Amit Sheth and Amitava Das},
      year={2023},
      eprint={2309.05922},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.05922}, 
}

@ARTICLE{llm.meet.search.1,
  author={Xiong, Haoyi and Bian, Jiang and Li, Yuchen and Li, Xuhong and Du, Mengnan and Wang, Shuaiqiang and Yin, Dawei and Helal, Sumi},
  journal={IEEE Transactions on Services Computing}, 
  title={When Search Engine Services Meet Large Language Models: Visions and Challenges}, 
  year={2024},
  volume={17},
  number={6},
  pages={4558-4577},
  keywords={Search engines;Accuracy;Training;Service computing;Indexing;Chatbots;Transformers;Large language models (LLMs);search engines;learning-to-rank (LTR);and retrieve-augmented generation (RAG)},
  doi={10.1109/TSC.2024.3451185}}

@article{llm.meet.search.2,
  title={Know where to go: Make LLM a relevant, responsible, and trustworthy searchers},
  author={Shi, Xiang and Liu, Jiawei and Liu, Yinpeng and Cheng, Qikai and Lu, Wei},
  journal={Decision Support Systems},
  volume={188},
  pages={114354},
  year={2025},
  publisher={Elsevier}
}

@misc{llm.meet.search.3,
      title={FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}, 
      author={Tu Vu and Mohit Iyyer and Xuezhi Wang and Noah Constant and Jerry Wei and Jason Wei and Chris Tar and Yun-Hsuan Sung and Denny Zhou and Quoc Le and Thang Luong},
      year={2023},
      eprint={2310.03214},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03214}, 
}

@article{advanced.search.necessity.1,
  title={Efficiency of Boolean search strings for Information retrieval},
  author={Aliyu, Muhammad Bello},
  journal={American Journal of Engineering Research},
  volume={6},
  number={11},
  pages={216--222},
  year={2017}
}

@INPROCEEDINGS{advanced.search.necessity.2,
  author={Raman, Shivangi and Vijay Kumar Chaurasiya and Venkatesan, S.},
  booktitle={2012 International Conference on Communication, Information \& Computing Technology (ICCICT)}, 
  title={Performance comparison of various information retrieval models used in search engines}, 
  year={2012},
  volume={},
  number={},
  pages={1-4},
  keywords={Search engines;Probabilistic logic;Web search;Vectors;Computational modeling;Analytical models;information retrieval;models;comparison;web search},
  doi={10.1109/ICCICT.2012.6398124}}

@article{llm.unstructured.data.1,
title = {Large language models overcome the challenges of unstructured text data in ecology},
journal = {Ecological Informatics},
volume = {82},
pages = {102742},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102742},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400284X},
author = {Andry Castro and João Pinto and Luís Reino and Pavel Pipek and César Capinha},
keywords = {AI, Automation, Data integration, GPT, LLaMA, Unstructured data},
abstract = {The vast volume of currently available unstructured text data, such as research papers, news, and technical report data, shows great potential for ecological research. However, manual processing of such data is labour-intensive, posing a significant challenge. In this study, we aimed to assess the application of three state-of-the-art prompt-based large language models (LLMs), GPT-3.5, GPT-4, and LLaMA-2-70B, to automate the identification, interpretation, extraction, and structuring of relevant ecological information from unstructured textual sources. We focused on species distribution data from two sources: news outlets and research papers. We assessed the LLMs for four key tasks: classification of documents with species distribution data, identification of regions where species are recorded, generation of geographical coordinates for these regions, and supply of results in a structured format. GPT-4 consistently outperformed the other models, demonstrating a high capacity to interpret textual data and extract relevant information, with the percentage of correct outputs often exceeding 90% (average accuracy across tasks: 87–100%). Its performance also depended on the data source type and task, with better results achieved with news reports, in the identification of regions with species reports and presentation of structured output. Its predecessor, GPT-3.5, exhibited slightly lower accuracy across all tasks and data sources (average accuracy across tasks: 81–97%), whereas LLaMA-2-70B showed the worst performance (37–73%). These results demonstrate the potential benefit of integrating prompt-based LLMs into ecological data assimilation workflows as essential tools to efficiently process large volumes of textual data.}
}

@misc{llm.unstructured.data.2,
      title={The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats}, 
      author={William Brach and Kristián Košťál and Michal Ries},
      year={2025},
      eprint={2503.02650},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.02650}, 
}

@misc{search.api.limit.1,
  author = {{Google Developers}},
  title = {Usage Limits},
  year = 2025,
  howpublished = {\url{https://support.google.com/programmable-search/answer/9069107?hl=en}},
  note = {Accessed: 28 July 2025}
}

@article{eniac.story,
 ISSN = {00304557},
 URL = {http://www.jstor.org/stable/45363261},
 author = {Martin H. Weik},
 journal = {Ordnance},
 number = {244},
 pages = {571--575},
 publisher = {National Defense Industrial Association},
 title = {The ENIAC Story},
 urldate = {2025-07-08},
 volume = {45},
 year = {1961}
}

@article{inverted.files.search,
author = {Zobel, Justin and Moffat, Alistair},
title = {Inverted files for text search engines},
year = {2006},
issue_date = {2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/1132956.1132959},
doi = {10.1145/1132956.1132959},
abstract = {The technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, index construction, and query evaluation. While some of these developments have been consolidated in textbooks, many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial, we introduce the key techniques in the area, describing both a core implementation and how the core can be enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexing literature.},
journal = {ACM Comput. Surv.},
month = jul,
pages = {6–es},
numpages = {56},
keywords = {text retrieval, information retrieval, document database, Web search engine, Inverted file indexing}
}

@book{intro.info.ret,
  title={Introduction to information retrieval},
  author={Sch{\"u}tze, Hinrich and Manning, Christopher D and Raghavan, Prabhakar},
  volume={39},
  year={2008},
  publisher={Cambridge University Press Cambridge}
}

@article{mapreduce,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: simplified data processing on large clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327492},
doi = {10.1145/1327452.1327492},
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
journal = {Commun. ACM},
month = jan,
pages = {107–113},
numpages = {7}
}

@INPROCEEDINGS{google.new.algos,
  author={Patil, Akshita and Pamnani, Jayesh and Pawade, Dipti},
  booktitle={2021 6th International Conference for Convergence in Technology (I2CT)}, 
  title={Comparative Study Of Google Search Engine Optimization Algorithms: Panda, Penguin and Hummingbird}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  keywords={Buildings;Search engines;Internet;Web sites;Optimization;Convergence;SEO;Panda;Penguin;Hummingbird;SERP},
  doi={10.1109/I2CT51068.2021.9418074}}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA},
  journal={OpenAI}
}

@misc{mit.software.construction,
	title={6.031: Software Construction},
	author={Goldman, Max and Miller, Rob},
	howpublished={\url{https://web.mit.edu/6.031/www/sp22/}},
	year={2022}
}

@article{unix.philosophy,
author = {Kernighan, Brian W. and Mashey, John R.},
title = {The UNIX™ programming environment},
journal = {Software: Practice and Experience},
volume = {9},
number = {1},
pages = {1-15},
keywords = {Operating systems, Programmer productivity},
doi = {https://doi.org/10.1002/spe.4380090102},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090102},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.4380090102},
abstract = {Abstract The UNIX UNIX is a trademark of Bell Laboratories. operating system provides an especially congenial programming environment, in which it is not only possible, but actually natural, to write programs quickly and well. Several characteristics of the UNIX system contribute to this desirable state of affairs. Files have no type or internal structure, so data produced by one program can be used by another without impediment. The basic system interface for input and output provides homogeneous treatment of files, I/O devices and programs, so programs need not care where their data comes from or goes to. The command interpreter makes it convenient to connect programs, by arranging for all data communication. Complex procedures are created not by writing large programs from scratch, but by interconnecting relatively small components. These programs are small and concentrate on single functions, and therefore are easy to build, understand, describe, and maintain. They form a high level toolkit whose existence causes programmers to view their work as the use and creation of tools, a viewpoint that encourages growth in place of reinvention. Tools interact in a limited number of ways, but can be used in many different combinations. Thus, an addition to the toolkit tends to improve the programming power of the user faster than it increases the complexity of interconnection and maintenance. Finally, tools are connected at a very high level by a powerful command language interpreter. The error-prone and expensive process of program writing can often be avoided in favor of program-using. In this paper we will present a variety of examples to illustrate this methodology, focusing on those aspects of the system and supporting software which make it possible.},
year = {1979}
}

@article{adt,
author = {Liskov, Barbara and Zilles, Stephen},
title = {Programming with abstract data types},
year = {1974},
issue_date = {April 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/942572.807045},
doi = {10.1145/942572.807045},
abstract = {The motivation behind the work in very-high-level languages is to ease the programming task by providing the programmer with a language containing primitives or abstractions suitable to his problem area. The programmer is then able to spend his effort in the right place; he concentrates on solving his problem, and the resulting program will be more reliable as a result. Clearly, this is a worthwhile goal.Unfortunately, it is very difficult for a designer to select in advance all the abstractions which the users of his language might need. If a language is to be used at all, it is likely to be used to solve problems which its designer did not envision, and for which the abstractions embedded in the language are not sufficient.This paper presents an approach which allows the set of built-in abstractions to be augmented when the need for a new data abstraction is discovered. This approach to the handling of abstraction is an outgrowth of work on designing a language for structured programming. Relevant aspects of this language are described, and examples of the use and definitions of abstractions are given.},
journal = {SIGPLAN Not.},
month = mar,
pages = {50–59},
numpages = {10}
}

@misc{openai.api.limit,
	author={OpenAI},
	title={API Rate Limits},
	howpublished={\url{https://platform.openai.com/docs/guides/rate-limits}},
	year={2025},
	note={Accessed 29 July 2025}
}
@misc{anthropic.api.limit,
	author={Anthropic},
	title={API Rate Limits},
	howpublished={\url{https://docs.anthropic.com/en/api/rate-limits}},
	year={2025},
	note={Accessed 29 July 2025}
}

@misc{ollama,
	author={{Ollama developers}},
	title={Ollama},
	howpublished={\url{https://ollama.com/}},
	year={2025},
	note={Accessed 29 July 2025}
}

@misc{ollama.models,
	author={{Ollama developers}},
	title={Search models},
	howpublished={\url{https://ollama.com/search}},
	year={2025},
	note={Accessed 29 July 2025}
}

@misc{llama3.1.bench.1,
      title={Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets}, 
      author={Tohida Rehman and Soumabha Ghosh and Kuntal Das and Souvik Bhattacharjee and Debarshi Kumar Sanyal and Samiran Chattopadhyay},
      year={2025},
      eprint={2502.19339},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.19339}, 
}
@misc{llama3.1.bench.2,
      title={Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs}, 
      author={Ankush Raut and Xiaofeng Zhu and Maria Leonor Pacheco},
      year={2025},
      eprint={2504.04745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.04745}, 
}
@misc{llama3.1.bench.3,
      title={Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks}, 
      author={Munief Hassan Tahir and Sana Shams and Layba Fiaz and Farah Adeeba and Sarmad Hussain},
      year={2024},
      eprint={2405.15453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.15453}, 
}

@misc{open.webui,
	author={Baek, Tim and {The Open WebUI Team}},
	title={Open WebUI},
	howpublished={\url{https://docs.openwebui.com/}},
	year={2025},
	note={Accessed 31 July 2025}
}

@inproceedings{rag,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{hybrid.method.1,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08909}, 
}

@misc{hybrid.method.2,
      title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering}, 
      author={Gautier Izacard and Edouard Grave},
      year={2021},
      eprint={2007.01282},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.01282}, 
}


@InProceedings{hybrid.method.3,
  title = 	 {Improving Language Models by Retrieving from Trillions of Tokens},
  author =       {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2206--2240},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/borgeaud22a.html},
  abstract = 	 {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25{\texttimes} fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.}
}

@article{hybrid.method.4,
title = {ReAct: Synergizing Reasoning and Acting in Language Models}, 
url = {https://par.nsf.gov/biblio/10451467},  
year = {2023},
journal = {International Conference on Learning Representations (ICLR)}, 
author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran,
		  Izhak and Narasimhan, Karthik and Cao, Yuan}, 
}

@misc{webgpt,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.09332}, 
}
