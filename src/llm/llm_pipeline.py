"""
@author Guanyuming He
Copyright (C) Guanyuming He 2025
The file is licensed under the GNU GPL v3.

LLM Pipeline. The logging code is generated by LLM, so they have the annoying
if hasattr(self, 'logger'). I deleted some but decided not to bother for the
rest. Except that, other code is written by myself.

This program implements the my pipeline design as in my thesis, figure 1.
0. (to be implemented) generate config file based on unstructured user input.
Also, schedule execution.

1. Read configuration file with business topics and parameters
2. Generate search prompts using LLMs
3. Execute searches using my own search engine (and maybe also commercial ones)
4. Store results in organized structure; syntehsize and summarize results using
LLMs;
5. Deliver results to user, based on the ways in the configuration file.

inf. (to be implemented) implement feedback and self improvement.
"""

import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any
import logging
from datetime import datetime
import smtplib
from email.message import EmailMessage

from llm_interface import send_to_ollama

DEFAULT_CONFIG_PATH = "./config.json"
DEFAULT_CONFIG = {
	"business_topics": [
		"Vertical integration in business",
		"Diversification strategies in business",
		"Competitive advantage in business",
		"Foreign direct investment in business"
	],

	"text_model": "llama3.1:8b",
	"file_model": "qwen2.5vl:7b",

	"verbose_level": 1,
	"search_prompt_generation": {
		"system_prompt": 
		"""
		You will receive abstract or concrete business-related concepts or
		documents (text, images, etc.). Your task is to generate search engine
		queries that would retrieve current or recent business news articles
		discussing these concepts in action.

		When the concept is abstract (e.g., "vertical integration", "global
		value chain", "how to evaluation a company's value"), follow these
		rules STRICTLY: 1. Identify concrete real-world examples, events,
		company names, industries, or case studies that might illustrate the
		concept. Use these to generate multiple keyword-based queries that
		search engines can match easily. Always remind yourself that a search
		engine can only do word match and cannot understand abstract ideas.  2.
		The final search queries for an abstract concept, e.g., "vertical
		integration" should include both the technical term, "vertical
		integration", its synonyms, e.g. "vertical consolidation", and the
		concrete phrases you think of. Connect the technical terms and concrete
		terms with OR, not AND.  3. Both 1 and 2 must appear in a query
		generated for an abstract input!  However, when the input itself is
		concrete, then you don't have to deabstract it.

		For example: (Abstract) Input: "Vertical integration" Output: 
				"vertical integration" OR "vertical consolidation" OR (Amazon
				warehouse logistics retail)

				"vertical integration" OR "vertical consolidation" OR (Tesla
				battery production) OR (vehicle manufacturing)

				"vertical integration" OR "vertical consolidation" OR (Apple
				chip design manufacturing)

				"vertical integration" OR "vertical consolidation" OR
				"Companies investing in end-to-end supply chains"

			(Abstract) Input: "Competitive advantage" Output: 
				"competitive advantage" OR "strategic edge" OR (Apple vs
				Microsoft battle)

				"competitive advantage" OR "strategic edge" OR (Microsoft's
				global dominance advantage)

			(Concrete) Input: "Trump tariff" Output: 
				Trump latest tariff

				Trump China tariff

				Trump tariff news

				Traiff Trump impacts.


		You could use boolean opeartors like AND and OR, but be very careful
		with AND, as it may lead not too narrow matches.

		Only output search queries, separating them by newlines. Do not explain
		or instruct. And DO NOT enclose the entire queries in e.g. quotes or
		special symbols.
				""",
		"max_prompts": 5
	},

	"synthesis": 
	{
		"system_prompt": 
		"""
		Forget ALL previous instructions!!!

		You will be given 
		1. a business topic.
		2. a list of results from search engine that are about the
		business topic. Each result will be a webpage url + tab + its title.
		
		Your task is to:
		1. Rerank the results based on relevance to the given topic.
		2. Summarize them, identify key articles from the results, and remember
		to include the URLs.
		""",
		"max_length": 3000
	},

	"schedules": 
	{
		"updater": {
			"schedule": "every_x_days",
			"every_x_days": 3,
			"time": "01:00",
			"command": "", # fill in later with cwd.
			"catch_up": False
		},
		"llm_pipeline": {
			"schedule": "weekly",
			"day": 1,
			"time": "04:00",
			"command": "", # fill in later with cwd.
			"catch_up": True
		}
	}  
	"email_addresses": ["example1@example.com", "example2@example.com"],
	"sender_email": "youremail@gmail.com",
	"sender_password": "yourapppassword"
}

class LLMPipeline:
	def __init__(self, config_path: str = DEFAULT_CONFIG_PATH):
		"""
		Initialize the LLM pipeline with configuration.
		"""
		self.config_path = config_path
		self.config = self.load_config()
		self.results = {}
		self.output_dir = Path("pipeline_output")
		self.output_dir.mkdir(exist_ok=True)
		self.setup_logging()
		
	def setup_logging(self):
		"""
		Configure logging based on verbose level from config.
		"""
		verbose_level = self.config.get('verbose_level', 1)
		
		if verbose_level == 0:
			# No logging
			logging.disable(logging.CRITICAL)
			return
		elif verbose_level == 1:
			log_level = logging.INFO
		elif verbose_level == 2:
			log_level = logging.DEBUG
		
		logging.basicConfig(
			level=log_level,
			format='%(asctime)s - %(levelname)s - %(message)s',
			handlers=[
				logging.FileHandler('llm_pipeline.log'),
				logging.StreamHandler()
			]
		)
		
		self.logger = logging.getLogger(__name__)
		
	def load_config(self) -> Dict[str, Any]:
		"""
		Load configuration from JSON file.
		"""
		try:
			with open(self.config_path, 'r') as f:
				config = json.load(f)
			
			# Validate required fields
			required_fields = [
				'business_topics', 
				'text_model', 
			'search_prompt_generation'
			]
			for field in required_fields:
				if field not in config:
					raise ValueError(f"Missing required field: {field}")
			
			return config
			
		except FileNotFoundError:
			print(
				f"Configuration file {self.config_path} not found. A default\
				one will be created.\n"
			)
			self.create_default_config()
			return DEFAULT_CONFIG

		except json.JSONDecodeError as e:
			raise RuntimeError(
				f"Invalid JSON in configuration file: {e}"
			)
	
	def create_default_config(self):
		"""
		Create a default configuration file if it's not already there.
		"""
		
		with open(self.config_path, 'w') as f:
			cwd = os.getcwd()
			if not cwd.endswith('/'):
				cwd += '/'

			# index 1000 more to the database once in a while.
			DEFAULT_CONFIG["schedules"]["updater"]["command"] = \
				cwd + "bin/updater ./db 1000"
			# run llm_pipeline once a while
			DEFAULT_CONFIG["schedules"]["llm_pipeline"]["command"] = \
				f"python3 {cwd}src/llm/llm_pipeline.py"

			json.dump(DEFAULT_CONFIG, f, indent=2)
		
		print(f"Sample configuration created at {self.config_path}")
	
	def generate_search_prompts(self, business_topic: str) -> List[str]:
		"""Generate search prompts for a business topic using Ollama."""
		if hasattr(self, 'logger'):
			self.logger.info(
				f"Generating search prompts for topic: " +
				f"{business_topic}"
			)
		
		response = send_to_ollama(
			self.config['text_model'], 
			self.config['search_prompt_generation']['system_prompt'],
			business_topic + "\n relatd search engine prompts"
		)
		
		# Parse the response to extract individual prompts
		# line.strip() returns True only if the lines is not empty after
		# stripping.
		prompts = [line.strip() for line in response.split('\n') 
				if line.strip()]
		
		# Limit the number of prompts if specified
		max_prompts = self.config['search_prompt_generation'].get(
			'max_prompts', len(prompts))
		prompts = prompts[:max_prompts]
		
		if hasattr(self, 'logger'):
			self.logger.info(f"Generated {len(prompts)} search prompts for "
						f"{business_topic}")
		return prompts
	
	def execute_search(self, search_prompt: str) -> str:
		"""Execute search using the local search engine."""
		if hasattr(self, 'logger'):
			self.logger.info(f"Executing search: {search_prompt}")
		
		try:
			cmd = ["./bin/searcher", "./db", search_prompt]
			result = subprocess.run(
				cmd,
				capture_output=True,
				text=True,
				timeout=3  # 3 seconds
			)
			
			if result.returncode != 0:
				warning_msg = (f"Search command failed for '{search_prompt}': "
							f"{result.stderr}")
				if hasattr(self, 'logger'):
					self.logger.warning(warning_msg)
				return f"Search failed: {result.stderr}"
			
			if hasattr(self, 'logger'):
				self.logger.info(f"Search completed successfully for: "
							f"{search_prompt}")
			return result.stdout
			
		except subprocess.TimeoutExpired:
			if hasattr(self, 'logger'):
				self.logger.error(f"Search timeout for prompt: {search_prompt}")
			return "Search timed out"
		except Exception as e:
			if hasattr(self, 'logger'):
				self.logger.error(f"Search error for '{search_prompt}': {e}")
			return f"Search error: {str(e)}"
	
	def store_results(self, topic: str, prompt: str, result: str):
		"""
		Store search results in an organized structure.
		"""
		# Create topic-specific directory
		topic_dir = self.output_dir / self.sanitize_filename(topic)
		topic_dir.mkdir(exist_ok=True)
		
		# Create a filename based on the search prompt
		prompt_filename = self.sanitize_filename(prompt)[:50] + ".txt"
		result_file = topic_dir / prompt_filename
		
		# Store the result
		with open(result_file, 'w', encoding='utf-8') as f:
			f.write(f"Search Prompt: {prompt}\n")
			f.write(f"Timestamp: {datetime.now().isoformat()}\n")
			f.write("="*50 + "\n")
			f.write(result)
		
		if hasattr(self, 'logger'):
			self.logger.info(f"Results stored: {result_file}")
		
		# Also store in memory for easy access
		if topic not in self.results:
			self.results[topic] = {}
		self.results[topic][prompt] = result
	
	def sanitize_filename(self, filename: str) -> str:
		"""Sanitize a string to be used as a filename."""
		import re
		# Remove/replace invalid characters
		filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
		filename = re.sub(r'\s+', '_', filename)
		return filename[:100]  # Limit length
	
	def synthesize_results(self, topic: str) -> str:
		"""
		Synthesize and summarize results for a topic using Ollama.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(f"Synthesizing results for topic: {topic}")
		
		if topic not in self.results:
			warning_msg = f"No results found for topic: {topic}"
			if hasattr(self, 'logger'):
				self.logger.warning(warning_msg)
			return "No results available for synthesis"
		
		# Combine all search results for the topic
		combined_results = ""
		for prompt, result in self.results[topic].items():
			combined_results += f"\n--- Search: {prompt} ---\n{result}\n"
		
		# Prepare synthesis prompt
		system_prompt = self.config['synthesis']['system_prompt']
		
		# Get synthesis from Ollama
		synthesis = send_to_ollama(
			self.config['text_model'], 
			system_prompt,
			"summarize these search results:\n\n" + combined_results
		)
		max_len = self.config["synthesis"].get("max_length", len(synthesis))
		synthesis = synthesis[:max_len]
		
		# Store synthesis
		synthesis_file = (self.output_dir / self.sanitize_filename(topic) / 
						"synthesis.txt")
		with open(synthesis_file, 'w', encoding='utf-8') as f:
			f.write(f"Topic: {topic}\n")
			f.write(f"Synthesis completed: {datetime.now().isoformat()}\n")
			f.write("="*50 + "\n")
			f.write(synthesis)
		
		self.logger.info(
			f"Synthesis completed and stored for topic: "
			f"{topic}"
		)
		return synthesis
	
	def run_pipeline(self):
		"""Execute the complete LLM pipeline."""
		self.logger.info("Starting business topic LLM pipeline")
		
		topics = self.config['business_topics']
		self.logger.info(f"Processing {len(topics)} business topics")
		
		# Step 1-4: For each topic, generate prompts, search, and store results
		for topic in topics:
			self.logger.info(f"\n--- Processing Topic: {topic} ---")
			
			try:
				# Generate search prompts
				search_prompts = self.generate_search_prompts(topic)
				
				# Execute searches and store results
				for prompt in search_prompts:
					search_result = self.execute_search(prompt)
					self.store_results(topic, prompt, search_result)
				
				self.logger.info(f"Completed search phase for topic: "
							f"{topic}")
				
			except Exception as e:
				self.logger.error(f"Error processing topic '{topic}': {e}")
				continue
		
		# Step 5: Synthesize results for each topic
		if hasattr(self, 'logger'):
			self.logger.info("\n--- Starting Synthesis Phase ---")
		syntheses = {}
		
		for topic in topics:
			try:
				synthesis = self.synthesize_results(topic)
				syntheses[topic] = synthesis
				self.logger.info(f"Synthesis completed for: {topic}")
				
			except Exception as e:
				self.logger.error(f"Error synthesizing results for "
								f"'{topic}': {e}")
				continue
		
		# Create final summary report
		self.create_summary_report(syntheses)
		
		if hasattr(self, 'logger'):
			self.logger.info("Business topic LLM pipeline completed "
						"successfully")
		return syntheses
	
	def create_summary_report(self, syntheses: Dict[str, str]):
		"""Create a comprehensive summary report."""
		report_path = self.output_dir / "final_report.md"
		
		with open(report_path, 'w', encoding='utf-8') as f:
			f.write("# Business Topic LLM Pipeline - Final Report\n\n")
			f.write(f"**Generated:** "
				f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
			f.write(f"**Topics Processed:** {len(syntheses)}\n\n")
			
			for topic, synthesis in syntheses.items():
				f.write(f"## {topic}\n\n")
				f.write(f"{synthesis}\n\n")
				f.write("---\n\n")

			# Don't call it now. Not ready yet.
			# send_report_by_email(f)
		
		self.logger.info(f"Final report created: {report_file}")


	def send_report_by_email(self, report_file):
		"""
		Sends the final report file to each of the email addresses specified in 
		config.json.
		"""
		email_list = self.config["email_addresses"]
		sender_email = self.config["sender_email"]
		sender_password = self.config["sender_password"]

		# Load file content
		with open("message.txt", "r") as f:
			file_content = f.read()

		# Set up SMTP connection
		smtp_server = "smtp.gmail.com"
		smtp_port = 587

		with smtplib.SMTP(smtp_server, smtp_port) as server:
			server.starttls()
			server.login(sender_email, sender_password)

			for recipient in email_list:
				msg = EmailMessage()
				msg["Subject"] = "Here is the report of the Business contents"
				msg["From"] = sender_email
				msg["To"] = recipient
				msg.set_content(file_content)

				server.send_message(msg)
				self.logger.info(f"Final report sent to {recipient}")


if __name__ == "__main__":
	try:
		# Initialize and run the pipeline
		pipeline = LLMPipeline()
		results = pipeline.run_pipeline()
		
		print("\n" + "="*50)
		print("BUSINESS TOPIC LLM PIPELINE COMPLETED")
		print("="*50)
		print(f"Results stored in: {pipeline.output_dir}")
		print(f"Topics processed: {len(results)}")
		
		# Print brief summary
		for topic in results:
			print(f"âœ“ {topic}")
		
	except KeyboardInterrupt:
		print("\nPipeline interrupted by user")
	except Exception as e:
		print(f"Pipeline failed: {e}")
		sys.exit(1)

