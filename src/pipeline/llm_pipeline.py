"""
@author Guanyuming He
Copyright (C) Guanyuming He 2025
The file is licensed under the GNU GPL v3.

LLM Pipeline. The logging code is generated by LLM, so they have the annoying
if hasattr(self, 'logger'). I deleted some but decided not to bother for the
rest. Except that, other code is written by myself.

This program implements the my pipeline design as in my thesis, figure 1.
0. (to be implemented) generate config file based on unstructured user input.
Also, schedule execution.

1. Read configuration file with business topics and parameters
2. Generate search prompts using LLMs
3. Execute searches using my own search engine (and maybe also commercial ones)
4. Store results in organized structure; syntehsize and summarize results using
LLMs;
5. Deliver results to user, based on the ways in the configuration file.

inf. (to be implemented) implement feedback and self improvement.
"""

import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any
import logging
import datetime
# for securely sending emails
import smtplib, ssl
from email.message import EmailMessage

# My local files
from llm_interface import send_to_ollama
from config import Config
from search_combined import search_filter_combine

DEFAULT_CONFIG_PATH = "./config.json"

class LLMPipeline:
	def __init__(self, config_path: str = DEFAULT_CONFIG_PATH):
		"""
		Initialize the LLM pipeline with configuration.
		"""
		self.config_path = config_path
		self.config : Config = self.load_config()

		self.results : dict = {}
		os.system("rm -rf ./pipeline_output")
		self.output_dir = Path("pipeline_output")
		self.output_dir.mkdir(exist_ok=True)

		self.setup_logging()
		
	def load_config(self) -> Config:
		"""
		Load configuration from JSON file.
		"""
		try:
			with open(self.config_path, 'r') as f:
				config:dict = json.load(f)
			
			# Validate required fields
			required_fields = [
				"business_topics", 
				"text_model", 
				"search_conf"
			]
			for field in required_fields:
				if field not in config:
					raise ValueError(f"Missing required field: {field}")
			return Config.from_dict(config)
			
		except FileNotFoundError:
			print(
				f"Configuration file {self.config_path} not found. A default\
				one will be created.\n"
			)
			ret = Config.load_default()
			ret.save_to(self.config_path)
			return ret
	
	def setup_logging(self):
		"""
		Configure logging based on verbose level from config.
		"""
		verbose_level = self.config.verbose_level
		
		if verbose_level == 0:
			# No logging
			logging.disable(logging.CRITICAL)
			return
		elif verbose_level == 1:
			log_level = logging.INFO
		elif verbose_level == 2:
			log_level = logging.DEBUG
		
		logging.basicConfig(
			level=log_level,
			format='%(asctime)s - %(levelname)s - %(message)s',
			handlers=[
				logging.FileHandler('llm_pipeline.log'),
				logging.StreamHandler()
			]
		)
		
		self.logger = logging.getLogger(__name__)
		
	def generate_search_prompts(self, business_topic: str) -> list[str]:
		"""
		Generate search prompts for a business topic using Ollama.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(
				f"Generating search prompts for topic: " +
				f"{business_topic}"
			)
		
		response_1 = send_to_ollama(
			self.config.text_model,
			self.config.search_conf.system_prompt_1,
			"Give subtopics for:\n" + business_topic
		)
		
		# Subtopics are put on individual lines.
		# line.strip() gives True only if the line is not empty after
		# stripping.
		subtopics = [
			line.strip() for line in response_1.split('\n') 
			if line.strip()
		]
		self.logger.info(
			f"Generated subtopics:\n {subtopics}"
		)

		prompts : list[str] = []
		n_st : int = 0
		for st in subtopics:
			response_2 = send_to_ollama(
				self.config.text_model,
				self.config.search_conf.system_prompt_2,
				"Give search query(ies) for:\n" + st
			)
			prompts += [
				line.strip() for line in response_2.split('\n')
				# Ignore the introductory lines the LLM may put in.
				# I also instruct it not to in the prompt,
				# but that won't always work.
				if (not "please go ahead" in line.lower()) and line.strip()
			][:self.config.search_conf.max_prompts_per_topic]
			
			n_st += 1
			if n_st >= self.config.search_conf.max_subtopics:
				break
		
		if hasattr(self, 'logger'):
			self.logger.info(f"Generated {len(prompts)} search prompts for "
						f"{business_topic}")
		return prompts
	
	def execute_search(self, search_prompt: str) -> str:
		"""
		Execute search using the local search engine.

		Assume that the pipeline's cwd is the project root.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(f"Executing search: {search_prompt}")
		
		ret : str = ""
		try:
			end_date = datetime.date.today()
			# by default, search for recent 8 months
			start_date = end_date - datetime.timedelta(days=240)
			ret = search_filter_combine(
				self.config.search_conf,
				search_prompt,
				start_date, end_date
			)
		except Exception as e:
			return f"Search error: {str(e)}"

		return ret
	
	def store_results(self, topic: str, prompt: str, result: str):
		"""
		Store search results in an organized structure.
		"""
		# Create topic-specific directory
		topic_dir = self.output_dir / self.sanitize_filename(topic)
		topic_dir.mkdir(exist_ok=True)
		
		# Create a filename based on the search prompt
		prompt_filename = self.sanitize_filename(prompt)[:50] + ".txt"
		result_file = topic_dir / prompt_filename
		
		# Store the result
		with open(result_file, 'w', encoding='utf-8') as f:
			f.write(f"Search Prompt: {prompt}\n")
			f.write(f"Timestamp: {datetime.datetime.now().isoformat()}\n")
			f.write("="*50 + "\n")
			f.write(result)
		
		if hasattr(self, 'logger'):
			self.logger.info(f"Results stored: {result_file}")
		
		# Also store in memory for easy access
		if topic not in self.results:
			self.results[topic] = {}
		self.results[topic][prompt] = result
	
	def sanitize_filename(self, filename: str) -> str:
		"""Sanitize a string to be used as a filename."""
		import re
		# Remove/replace invalid characters
		filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
		filename = re.sub(r'\s+', '_', filename)
		return filename[:100]  # Limit length
	
	def synthesize_results(self, topic: str) -> str:
		"""
		Synthesize and summarize results for a topic using Ollama.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(f"Synthesizing results for topic: {topic}")
		
		if topic not in self.results:
			warning_msg = f"No results found for topic: {topic}"
			if hasattr(self, 'logger'):
				self.logger.warning(warning_msg)
			return "No results available for synthesis"
		
		# Combine all search results for the topic
		combined_results = ""
		for prompt, result in self.results[topic].items():
			combined_results += f"\n--- Search: {prompt} ---\n{result}\n"

		topic_line : str = \
			f"*Business topic: {topic}*\n\n\n"
		
		# Stage 1: filter and rerank.
		filtered = send_to_ollama(
			self.config.text_model, 
			self.config.synthesis_conf.system_prompt_1,
			topic_line +
			"search results:\n" + combined_results
		)

		# Stage 2: summarize
		synthesis = send_to_ollama(
			self.config.text_model, 
			self.config.synthesis_conf.system_prompt_2,
			topic_line +
			"search results:\n" + filtered
		)
		max_len = min(
			self.config.synthesis_conf.max_len,
			len(synthesis)
		)
		synthesis = synthesis[:max_len]
		
		# Store synthesis
		synthesis_file = (self.output_dir / self.sanitize_filename(topic) / 
						"synthesis.txt")
		with open(synthesis_file, 'w', encoding='utf-8') as f:
			f.write(f"Topic: {topic}\n")
			f.write(f"Synthesis completed: {datetime.datetime.now().isoformat()}\n")
			f.write("="*50 + "\n")
			f.write(synthesis)
		
		self.logger.info(
			f"Synthesis completed and stored for topic: "
			f"{topic}"
		)
		return synthesis
	
	def run_pipeline(self):
		"""
		Execute the complete LLM pipeline.
		"""
		self.logger.info("Starting business topic LLM pipeline")
		
		topics = self.config.business_topics
		self.logger.info(f"Processing {len(topics)} business topics")
		
		# Step 1-4: For each topic, generate prompts, search, and store results
		for topic in topics:
			self.logger.info(f"\n--- Processing Topic: {topic} ---")
			
			try:
				# Generate search prompts
				search_prompts = self.generate_search_prompts(topic)
				
				# Execute searches and store results
				for prompt in search_prompts:
					search_result = self.execute_search(prompt)
					self.store_results(topic, prompt, search_result)
				
				self.logger.info(f"Completed search phase for topic: "
							f"{topic}")
				
			except Exception as e:
				self.logger.error(f"Error processing topic '{topic}': {e}")
				continue
		
		# Step 5: Synthesize results for each topic
		if hasattr(self, 'logger'):
			self.logger.info("\n--- Starting Synthesis Phase ---")
		syntheses = {}
		
		for topic in topics:
			try:
				synthesis = self.synthesize_results(topic)
				syntheses[topic] = synthesis
				self.logger.info(f"Synthesis completed for: {topic}")
				
			except Exception as e:
				self.logger.error(f"Error synthesizing results for "
								f"'{topic}': {e}")
				continue
		
		# Create final summary report
		self.create_summary_report(syntheses)
		
		if hasattr(self, 'logger'):
			self.logger.info("Business topic LLM pipeline completed "
						"successfully")
		return syntheses
	
	def create_summary_report(self, syntheses: Dict[str, str]):
		"""Create a comprehensive summary report."""
		report_path = self.output_dir / "final_report.md"
		
		with open(report_path, "w+", encoding="utf-8") as f:
			f.write("# Business Topic LLM Pipeline - Final Report\n\n")
			f.write(f"**Generated:** "
				f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
			f.write(f"**Topics Processed:** {len(syntheses)}\n\n")
			
			for topic, synthesis in syntheses.items():
				f.write(f"## {topic}\n\n")
				f.write(f"{synthesis}\n\n")
				f.write("---\n\n")

			self.send_report_by_email(f)
		
		self.logger.info(f"Final report created: {report_path}")


	def send_report_by_email(self, report_file):
		"""
		Sends the final report file to each of the email addresses specified in 
		config.json.
		"""
		email_list = self.config.email_info.dst_addresses
		sender_email = self.config.email_info.src_address
		sender_password = self.config.email_info.src_passwd

		# Load file content
		report_file.seek(0)
		file_content = report_file.read()

		# Set up SMTP connection
		smtp_server = self.config.email_info.src_provider
		smtp_port = 587

		# This will use the system's settings, in particular, the
		# ca-certificates.
		context = ssl.create_default_context()
		with smtplib.SMTP(smtp_server, smtp_port) as server:
			# According to the SMTP protocol, I need to identify myself to the
			# server with this. It also needs to be done again, after TLS
			# setup.
			# But the two calls are optional, since send_mail automatically
			# calls it if not already called. I favor explicitness, so I call
			# them.
			server.ehlo()
			server.starttls(context=context)
			server.ehlo()
			server.login(sender_email, sender_password)

			for recipient in email_list:
				msg = EmailMessage()
				msg["Subject"] = "Here is the report of the Business contents"
				msg["From"] = sender_email
				msg["To"] = recipient
				msg.set_content(file_content)

				server.send_message(msg)
				self.logger.info(f"Final report sent to {recipient}")


if __name__ == "__main__":
	try:
		# Initialize and run the pipeline
		pipeline = LLMPipeline()
		results = pipeline.run_pipeline()
		
		print("\n" + "="*50)
		print("BUSINESS TOPIC LLM PIPELINE COMPLETED")
		print("="*50)
		print(f"Results stored in: {pipeline.output_dir}")
		print(f"Topics processed: {len(results)}")
		
		# Print brief summary
		for topic in results:
			print(f"✓ {topic}")
		
	except KeyboardInterrupt:
		print("\nPipeline interrupted by user")
	except Exception as e:
		print(f"Pipeline failed: {e}")
		sys.exit(1)

