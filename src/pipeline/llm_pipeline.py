"""
@author Guanyuming He
Copyright (C) Guanyuming He 2025
The file is licensed under the GNU GPL v3.

LLM Pipeline. The logging code is generated by LLM, so they have the annoying
if hasattr(self, 'logger'). I deleted some but decided not to bother for the
rest. Except that, other code is written by myself.

This program implements the my pipeline design as in my thesis, figure 1.
0. (to be implemented) generate config file based on unstructured user input.
Also, schedule execution.

1. Read configuration file with business topics and parameters
2. Generate search prompts using LLMs
3. Execute searches using my own search engine (and maybe also commercial ones)
4. Store results in organized structure; syntehsize and summarize results using
LLMs;
5. Deliver results to user, based on the email addresses in the configuration
file.

"""

import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any
import logging
import datetime
# for securely sending emails
import smtplib, ssl
from email.message import EmailMessage
import re

# My local files
from llm_interface import send_to_ollama
from config import Config
from search_combined import search_filter_combine

DEFAULT_CONFIG_PATH = "./config.json"

class LLMPipeline:
	def __init__(self, config_path: str = DEFAULT_CONFIG_PATH):
		"""
		Initialize the LLM pipeline with configuration.
		"""
		self.config_path = config_path
		self.config : Config = self.load_config()

		self.results : dict = {}
		os.system("rm -rf ./pipeline_output")
		self.output_dir = Path("pipeline_output")
		self.output_dir.mkdir(exist_ok=True)

		self.setup_logging()
		
	def load_config(self) -> Config:
		"""
		Load configuration from JSON file.
		"""
		try:
			with open(self.config_path, 'r') as f:
				config:dict = json.load(f)
			
			# Validate required fields
			required_fields = [
				"business_topics", 
				"text_model", 
				"search_conf"
			]
			for field in required_fields:
				if field not in config:
					raise ValueError(f"Missing required field: {field}")
			return Config.from_dict(config)
			
		except FileNotFoundError:
			print(
				f"Configuration file {self.config_path} not found. A default\
				one will be created.\n"
			)
			ret = Config.load_default()
			ret.save_to(self.config_path)
			return ret
	
	def setup_logging(self):
		"""
		Configure logging based on verbose level from config.
		"""
		verbose_level = self.config.verbose_level
		
		if verbose_level == 0:
			# No logging
			logging.disable(logging.CRITICAL)
			return
		elif verbose_level == 1:
			log_level = logging.INFO
		elif verbose_level == 2:
			log_level = logging.DEBUG
		
		logging.basicConfig(
			level=log_level,
			format='%(asctime)s - %(levelname)s - %(message)s',
			handlers=[
				logging.FileHandler('llm_pipeline.log'),
				logging.StreamHandler()
			]
		)
		
		self.logger = logging.getLogger(__name__)
		
	def generate_search_prompts(self, business_topic: str) -> list[str]:
		"""
		Generate search prompts for a business topic using Ollama.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(
				f"Generating search prompts for topic: " +
				f"{business_topic}"
			)
		
		response_1 = send_to_ollama(
			self.config.text_model,
			self.config.search_conf.system_prompt_1,
			"Give subtopics for:\n" + business_topic
		)
		
		# Subtopics are put on individual lines.
		# line.strip() gives True only if the line is not empty after
		# stripping.
		subtopics = [
			line.strip() for line in response_1.split('\n') 
			if line.strip()
		]
		self.logger.info(
			f"Generated subtopics:\n {subtopics}"
		)

		prompts : list[str] = []
		n_st : int = 0
		for st in subtopics:
			response_2 = send_to_ollama(
				self.config.text_model,
				self.config.search_conf.system_prompt_2,
				"Give search query(ies) for:\n" + st
			)
			prompts += [
				line.strip() for line in response_2.split('\n')
				# Ignore the introductory lines the LLM may put in.
				# I also instruct it not to in the prompt,
				# but that won't always work.
				if (not "please go ahead" in line.lower()) and line.strip()
			][:self.config.search_conf.max_prompts_per_topic]
			
			n_st += 1
			if n_st >= self.config.search_conf.max_subtopics:
				break
		
		if hasattr(self, 'logger'):
			self.logger.info(f"Generated {len(prompts)} search prompts for "
						f"{business_topic}")
		return prompts
	
	def execute_search(self, search_prompt: str) -> str:
		"""
		Execute search using the local search engine.

		Assume that the pipeline's cwd is the project root.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(f"Executing search: {search_prompt}")
		
		ret : str = ""
		try:
			end_date = datetime.date.today()
			# by default, search for recent 8 months
			start_date = end_date - datetime.timedelta(days=240)
			ret = search_filter_combine(
				self.config.search_conf,
				search_prompt,
				start_date, end_date
			)
		except Exception as e:
			return f"Search error: {str(e)}"

		return ret
	
	def store_results(self, topic: str, prompt: str, result: str):
		"""
		Store search results in an organized structure.
		"""
		# Create topic-specific directory
		topic_dir = self.output_dir / self.sanitize_filename(topic)
		topic_dir.mkdir(exist_ok=True)
		
		# Create a filename based on the search prompt
		prompt_filename = self.sanitize_filename(prompt)[:50] + ".txt"
		result_file = topic_dir / prompt_filename
		
		# Store the result
		with open(result_file, 'w', encoding='utf-8') as f:
			f.write(f"Search Prompt: {prompt}\n")
			f.write(f"Timestamp: {datetime.datetime.now().isoformat()}\n")
			f.write("="*50 + "\n")
			f.write(result)
		
		if hasattr(self, 'logger'):
			self.logger.info(f"Results stored: {result_file}")
		
		# Also store in memory for easy access
		if topic not in self.results:
			self.results[topic] = {}
		self.results[topic][prompt] = result
	
	def sanitize_filename(self, filename: str) -> str:
		"""Sanitize a string to be used as a filename."""
		# Remove/replace invalid characters
		filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
		filename = re.sub(r'\s+', '_', filename)
		return filename[:100]  # Limit length
	
	def synthesize_results(self, topic: str) -> str:
		"""
		Synthesize and summarize results for a topic using Ollama.
		Two stages:
		1. LLM filter and rerank each individual search result. I do not decide
		to output them all to LLM and let LLM output the filtered, which can be
		distorted or lost. I will simply let LLM output a scale
		0--10, < 4 = not relevant
		and then use that to filter and rerank myself.
		2. Put the filtered and reranked to LLM to summarize.
		"""
		if hasattr(self, 'logger'):
			self.logger.info(f"Synthesizing results for topic: {topic}")
		
		if topic not in self.results:
			warning_msg = f"No results found for topic: {topic}"
			if hasattr(self, 'logger'):
				self.logger.warning(warning_msg)
			return "No results available for synthesis"
		
		topic_line : str = \
			f"Topic: {topic}\n\n"

		# Stage 1: filter and rerank
		#
		# Filtered res, mapping res to relevance number.
		# Contains all filtered results for the whole topic.
		dict_res = dict()
		num_total_res = 0
		for prompt, result in self.results[topic].items():
			# which I need to separate manually.
			# Each result is actually a collection of results for a prompt
			# Each two are separated by an empty line, so split with \n\n.
			list_res = result.split("\n\n")
			num_total_res += len(list_res)


			# The filtering threshold depends on how many results are
			# returned.
			# The values here are self tuned, because the LLM seems to output
			# little when there is little, and many when there are many.
			threshold : float = 5.0
			if len(list_res) <= 5:
				# include all if less than 5 results
				threshold = 0.0
			elif len(list_res) <= 50:
				threshold = float(len(list_res)) / 10.0
			else:
				threshold = 5.0

			for r in list_res:
				# Probably Google's search engine out of quota.
				# Better ignore
				if r.startswith("Search error"):
					continue

				# The system prompt will ask the LLM to send out
				# a 0--10 score based on the result
				rel_score = send_to_ollama(
					self.config.text_model,
					self.config.synthesis_conf.system_prompt_1,
					f"{topic_line}Result:\n{r}"
				)
				# Find the first real number or integer from rel_score,
				# because the LLM might still output leading bullshit, even if
				# I tell it not to from system prompt.
				num : float = 0.0
				num_str : str = ""
				match = re.search(r'(?:\d+\.\d+|\.\d+|\d+)', rel_score)
				# sometimes LLM will fuckup and output no number.
				# check that out.
				if match is not None: 
					num_str = match.group()
					num = float(num_str) if '.' in num_str else int(num_str)

				if num >= threshold:
					dict_res[r] = num

		self.logger.info(
			f"Filter: {len(dict_res)} remains out of {num_total_res}."
		)

		# Now, rerank based on the relevance score
		sorted_res = sorted(dict_res, key=dict_res.get)
		sorted_combined = ""
		for r in sorted_res:
			sorted_combined += r
			sorted_combined += '\n'

		# I could store them in another file to compare with the original all
		# results.
		topic_dir = self.output_dir / self.sanitize_filename(topic)
		filtered_path = topic_dir / "filtered_results.txt"
		with open(filtered_path, 'w', encoding="utf-8") as f:
			f.write(f"Filtered results for {topic}\n")
			f.write("="*50 + "\n")
			for res in sorted_res:
				f.write(f"Score: {dict_res.get(res)}\n")
				f.write(res)
				f.write('\n')


		# Stage 2: summarize
		synthesis = send_to_ollama(
			self.config.text_model, 
			self.config.synthesis_conf.system_prompt_2,
			topic_line +
			"search results:\n" + sorted_combined
		)
		max_len = min(
			self.config.synthesis_conf.max_len,
			len(synthesis)
		)
		synthesis = synthesis[:max_len]
		
		# Store synthesis
		synthesis_file = (self.output_dir / self.sanitize_filename(topic) / 
						"synthesis.txt")
		with open(synthesis_file, 'w', encoding='utf-8') as f:
			f.write(f"Topic: {topic}\n")
			f.write(f"Synthesis completed: {datetime.datetime.now().isoformat()}\n")
			f.write("="*50 + "\n")
			f.write(synthesis)
		
		self.logger.info(
			f"Synthesis completed and stored for topic: "
			f"{topic}"
		)
		return synthesis
	
	def run_pipeline(self):
		"""
		Execute the complete LLM pipeline.
		"""
		self.logger.info("Starting business topic LLM pipeline")
		
		topics = self.config.business_topics
		self.logger.info(f"Processing {len(topics)} business topics")
		
		# Step 1-4: For each topic, generate prompts, search, and store results
		for topic in topics:
			self.logger.info(f"\n--- Processing Topic: {topic} ---")
			
			try:
				# Generate search prompts
				search_prompts = self.generate_search_prompts(topic)
				
				# Execute searches and store results
				for prompt in search_prompts:
					search_result = self.execute_search(prompt)
					self.store_results(topic, prompt, search_result)
				
				self.logger.info(f"Completed search phase for topic: "
							f"{topic}")
				
			except Exception as e:
				self.logger.error(f"Error processing topic '{topic}': {e}")
				continue
		
		# Step 5: Synthesize results for each topic
		if hasattr(self, 'logger'):
			self.logger.info("\n--- Starting Synthesis Phase ---")
		syntheses = {}
		
		for topic in topics:
			try:
				synthesis = self.synthesize_results(topic)
				syntheses[topic] = synthesis
				self.logger.info(f"Synthesis completed for: {topic}")
				
			except Exception as e:
				self.logger.error(f"Error synthesizing results for "
								f"'{topic}': {e}")
				continue
		
		# Create final summary report
		self.create_summary_report(syntheses)
		
		if hasattr(self, 'logger'):
			self.logger.info("Business topic LLM pipeline completed "
						"successfully")
		return syntheses
	
	def create_summary_report(self, syntheses: Dict[str, str]):
		"""Create a comprehensive summary report."""
		report_path = self.output_dir / "final_report.md"
		
		with open(report_path, "w+", encoding="utf-8") as f:
			f.write("# Business Topic LLM Pipeline - Final Report\n\n")
			f.write(f"**Generated:** "
				f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
			f.write(f"**Topics Processed:** {len(syntheses)}\n\n")
			
			for topic, synthesis in syntheses.items():
				f.write(f"## {topic}\n\n")
				f.write(f"{synthesis}\n\n")
				f.write("---\n\n")

			self.send_report_by_email(f)
		
		self.logger.info(f"Final report created: {report_path}")


	def send_report_by_email(self, report_file):
		"""
		Sends the final report file to each of the email addresses specified in 
		config.json.
		"""
		email_list = self.config.email_info.dst_addresses
		sender_email = self.config.email_info.src_address
		sender_password = self.config.email_info.src_passwd

		# Load file content
		report_file.seek(0)
		file_content = report_file.read()

		# Set up SMTP connection
		smtp_server = self.config.email_info.src_provider
		smtp_port = 587

		# This will use the system's settings, in particular, the
		# ca-certificates.
		context = ssl.create_default_context()
		with smtplib.SMTP(smtp_server, smtp_port) as server:
			# According to the SMTP protocol, I need to identify myself to the
			# server with this. It also needs to be done again, after TLS
			# setup.
			# But the two calls are optional, since send_mail automatically
			# calls it if not already called. I favor explicitness, so I call
			# them.
			server.ehlo()
			server.starttls(context=context)
			server.ehlo()
			server.login(sender_email, sender_password)

			for recipient in email_list:
				msg = EmailMessage()
				msg["Subject"] = "Here is the report of the Business contents"
				msg["From"] = sender_email
				msg["To"] = recipient
				msg.set_content(file_content)

				server.send_message(msg)
				self.logger.info(f"Final report sent to {recipient}")


if __name__ == "__main__":
	try:
		# Initialize and run the pipeline
		pipeline = LLMPipeline()
		results = pipeline.run_pipeline()
		
		print("\n" + "="*50)
		print("BUSINESS TOPIC LLM PIPELINE COMPLETED")
		print("="*50)
		print(f"Results stored in: {pipeline.output_dir}")
		print(f"Topics processed: {len(results)}")
		
		# Print brief summary
		for topic in results:
			print(f"âœ“ {topic}")
		
	except KeyboardInterrupt:
		print("\nPipeline interrupted by user")
	except Exception as e:
		print(f"Pipeline failed: {e}")
		sys.exit(1)

